{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tabular Playground Series - Aug 2022 - Top 2% - 24/1888","metadata":{}},{"cell_type":"code","source":"!pip install feature_engine\n!git clone https://github.com/analokmaus/kuma_utils.git\nimport sys; sys.path.append(\"kuma_utils/\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport sys\nimport joblib\nimport numpy as np\nimport pandas as pd\nimport gc; gc.enable()\nfrom lightgbm import LGBMClassifier\nfrom sklearn.impute import KNNImputer\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom feature_engine.encoding import WoEEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom kuma_utils.preprocessing.imputer import LGBMImputer\nfrom sklearn.linear_model import LogisticRegression, HuberRegressor\nimport warnings; warnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Import data**","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/tabular-playground-series-aug-2022/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/tabular-playground-series-aug-2022/test.csv\")\nsub = pd.read_csv(\"/kaggle/input/tabular-playground-series-aug-2022/sample_submission.csv\")\ntarget, groups = df_train['failure'], df_train['product_code']\ndf_train.drop('failure',axis=1, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Preprocessing, thanks also to the other competitors**","metadata":{}},{"cell_type":"code","source":"def preprocessing(df_train, df_test):\n    data = pd.concat([df_train, df_test])\n    \n    data['m3_missing'] = data['measurement_3'].isnull().astype(np.int8)\n    data['m5_missing'] = data['measurement_5'].isnull().astype(np.int8)\n    data['area'] = data['attribute_2'] * data['attribute_3']\n\n    feature = [f for f in df_test.columns if f.startswith('measurement') or f=='loading']\n\n    full_fill_dict ={}\n    full_fill_dict['measurement_17'] = {\n        'A': ['measurement_5','measurement_6','measurement_8'],\n        'B': ['measurement_4','measurement_5','measurement_7'],\n        'C': ['measurement_5','measurement_7','measurement_8','measurement_9'],\n        'D': ['measurement_5','measurement_6','measurement_7','measurement_8'],\n        'E': ['measurement_4','measurement_5','measurement_6','measurement_8'],\n        'F': ['measurement_4','measurement_5','measurement_6','measurement_7'],\n        'G': ['measurement_4','measurement_6','measurement_8','measurement_9'],\n        'H': ['measurement_4','measurement_5','measurement_7','measurement_8','measurement_9'],\n        'I': ['measurement_3','measurement_7','measurement_8']\n    }\n\n    col = [col for col in df_test.columns if 'measurement' not in col]+ ['loading','m3_missing','m5_missing']\n    a = []\n    b =[]\n    for x in range(3,17):\n        corr = np.absolute(data.drop(col, axis=1).corr()[f'measurement_{x}']).sort_values(ascending=False)\n        a.append(np.round(np.sum(corr[1:4]),3))\n        b.append(f'measurement_{x}')\n    c = pd.DataFrame()\n    c['Selected columns'] = b\n    c['correlation total'] = a\n    c = c.sort_values(by = 'correlation total',ascending=False).reset_index(drop = True)\n    display(c.head(10))\n\n    for i in range(10):\n        measurement_col = 'measurement_' + c.iloc[i,0][12:]\n        fill_dict = {}\n        for x in data.product_code.unique() : \n            corr = np.absolute(data[data.product_code == x].drop(col, axis=1).corr()[measurement_col]).sort_values(ascending=False)\n            measurement_col_dic = {}\n            measurement_col_dic[measurement_col] = corr[1:5].index.tolist()\n            fill_dict[x] = measurement_col_dic[measurement_col]\n        full_fill_dict[measurement_col] =fill_dict\n\n    feature = [f for f in data.columns if f.startswith('measurement') or f=='loading']\n    nullValue_cols = [col for col in df_train.columns if df_train[col].isnull().sum()!=0]\n\n    for code in data.product_code.unique():\n        total_na_filled_by_linear_model = 0\n        for measurement_col in list(full_fill_dict.keys()):\n            tmp = data[data.product_code == code]\n            column = full_fill_dict[measurement_col][code]\n            tmp_train = tmp[column+[measurement_col]].dropna(how='any')\n            tmp_test = tmp[(tmp[column].isnull().sum(axis=1)==0)&(tmp[measurement_col].isnull())]\n\n            model = HuberRegressor(epsilon=1.9)\n            model.fit(tmp_train[column], tmp_train[measurement_col])\n            data.loc[(data.product_code==code)&(data[column].isnull().sum(axis=1)==0)&(data[measurement_col].isnull()),measurement_col] = model.predict(tmp_test[column])\n            total_na_filled_by_linear_model += len(tmp_test)\n            \n        NA = data.loc[data[\"product_code\"] == code,nullValue_cols ].isnull().sum().sum()\n        model1 = KNNImputer(n_neighbors=3)\n        data.loc[data.product_code==code, feature] = model1.fit_transform(data.loc[data.product_code==code, feature])\n        \n    data['measurement_avg'] = data[[f'measurement_{i}' for i in range(3, 17)]].mean(axis=1)\n    df_train = data.iloc[:df_train.shape[0],:]\n    df_test = data.iloc[df_train.shape[0]:,:]\n\n    woe_encoder = WoEEncoder(variables=['attribute_0'])\n    woe_encoder.fit(df_train, target)\n    df_train = woe_encoder.transform(df_train)\n    df_test = woe_encoder.transform(df_test)\n\n    features = ['loading', 'attribute_0', 'measurement_17', 'measurement_0', 'measurement_1', 'measurement_2', 'area', 'm3_missing', 'm5_missing', 'measurement_avg']\n    \n    return df_train, df_test, features\n\ndef scale(train_data, val_data, test_data, feats):\n    scaler = StandardScaler()\n    scaled_train = scaler.fit_transform(train_data[feats])\n    scaled_val = scaler.transform(val_data[feats])\n    scaled_test = scaler.transform(test_data[feats])\n    new_train = train_data.copy()\n    new_val = val_data.copy()\n    new_test = test_data.copy()\n    new_train[feats] = scaled_train\n    new_val[feats] = scaled_val\n    new_test[feats] = scaled_test\n    return new_train, new_val, new_test\n\ndf_train, df_test, features = preprocessing(df_train, df_test)\ndf_train['failure'] = target","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model**","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression, BayesianRidge, Ridge\nimport tensorflow as tf\n\noof = np.zeros(len(df_train))\n\nresult = []\nimportance_list = []\n\ntest_preds = np.zeros(len(df_test))\nfor fold, (train_idx, val_idx) in enumerate(StratifiedKFold(n_splits = 5, shuffle = True, random_state = 0).split(df_train, df_train[\"failure\"])):\n    x_train, y_train = df_train.loc[train_idx][features], df_train.loc[train_idx][\"failure\"]\n    x_val, y_val = df_train.loc[val_idx][features], df_train.loc[val_idx][\"failure\"]\n\n    scaler = StandardScaler()\n    x_train_scaled = scaler.fit_transform(x_train)\n    x_val_scaled = scaler.transform(x_val)\n    test_scaled = scaler.transform(df_test[features].copy())\n    \n    model = LogisticRegression(max_iter= 500, C=0.0001, penalty=\"l2\", solver=\"newton-cg\")\n    lgb_params = {\n        'seed': 0,\n        'n_jobs': -1,\n        'lambda_l2': 2,\n        'metric': \"auc\",\n        'max_depth': -1,\n        'num_leaves': 100,\n        'boosting': 'gbdt',\n        'bagging_freq': 20,\n        'learning_rate': 0.01,\n        'objective': 'binary',\n        'min_data_in_leaf': 50,\n        'num_boost_round': 500,\n        'feature_fraction': 0.90,\n        'bagging_fraction': 0.90,\n    }\n    model1 = LGBMClassifier(**lgb_params)\n    \n    model.fit(x_train, y_train)\n    model1.fit(x_train_scaled, y_train)\n    \n    y_pred0 = model.predict_proba(x_val)[:, 1]   \n    y_pred1 = model1.predict_proba(x_val_scaled)[:, 1]    \n    y_pred = y_pred0*0.90+y_pred1*0.10\n        \n    r = roc_auc_score(y_val, y_pred)\n    \n    importance_list.append(model.coef_.ravel())\n    result.append(r)\n    \n    pred = model.predict_proba(df_test[features])[:, 1]*r    \n    pred1 = model1.predict_proba(test_scaled)[:, 1]*r\n    \n    test_preds += pred*0.90+pred1*0.10\n    \n    oof[val_idx] = y_pred\n    print(f\"Val score: {roc_auc_score(y_val, y_pred):.7f}\")\n\nprint(f\"Val score: {roc_auc_score(df_train['failure'], oof):.7f}\")\nprint(\"Mean\", round(sum(result)/5,7) )\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Output**","metadata":{}},{"cell_type":"code","source":"output = pd.read_csv('../input/tabular-playground-series-aug-2022/sample_submission.csv')\noutput['failure'] = test_preds/sum(result)\noutput.to_csv('pred19.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output[\"failure\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}